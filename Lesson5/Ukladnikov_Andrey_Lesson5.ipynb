{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Урок 5. Сверточные нейронные сети для анализа текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ЗАДАНИЕ 1. \n",
    "\n",
    "Учим conv сеть для классификации - выбить auc выше 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\windows\\anaconda3\\envs\\python38\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\windows\\appdata\\roaming\\python\\python38\\site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: h5py in c:\\users\\windows\\anaconda3\\envs\\python38\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\windows\\appdata\\roaming\\python\\python38\\site-packages (from keras) (1.18.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\windows\\anaconda3\\envs\\python38\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: six in c:\\users\\windows\\anaconda3\\envs\\python38\\lib\\site-packages (from h5py->keras) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Embedding, Conv1D, GlobalMaxPool1D,SpatialDropout1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.callbacks import EarlyStopping  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text  class\n",
      "0   0  @alisachachka не уезжаааааааай. :(❤ я тоже не ...      0\n",
      "1   1  RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...      1\n",
      "2   2  RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...      0\n",
      "3   3  RT @epupybobv: Хочется котлету по-киевски. Зап...      1\n",
      "4   4  @KarineKurganova @Yess__Boss босапопа есбоса н...      1\n",
      "       id                                               text\n",
      "0  204150   Тектоника и рельеф-самое ужасное в мире мучение(\n",
      "1  204151  Ходили запускать шар желаний, но у нас не полу...\n",
      "2  204152  Хочу лето только ради того, что бы направить н...\n",
      "3  204153  RT @RonyLiss: @colf_ne блин((\\nа я шипперила Ф...\n",
      "4  204154  RT @anna_romt: @ZADROT_PO_IGRAM блин,каждое во...\n",
      "       id                                               text  class\n",
      "0  181467  RT @TukvaSociopat: Максимальный репост! ))) #є...      1\n",
      "1  181468  чтоб у меня з.п. ежегодно индексировали на инд...      0\n",
      "2  181469        @chilyandlime нехуя мне не хорошо !!! :((((      0\n",
      "3  181470  @inafish нее , когда ногами ахахах когда?ахаха...      0\n",
      "4  181471  Хочу сделать как лучше,  а получаю как всегда. :(      0\n"
     ]
    }
   ],
   "source": [
    "max_words = 200\n",
    "max_len = 40\n",
    "num_classes = 1\n",
    "\n",
    "# Training\n",
    "epochs = 20\n",
    "batch_size = 512\n",
    "print_batch_n = 100\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_val = pd.read_csv(\"data/val.csv\")\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n",
    "print(df_val.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Windows\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re\n",
    "\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"\\sне\", \"не\", txt)\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
    "    return \" \".join(txt)\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "df_val['text'] = df_val['text'].apply(preprocess_text)\n",
    "df_test['text'] = df_test['text'].apply(preprocess_text)\n",
    "\n",
    "train_corpus = \" \".join(df_train[\"text\"])\n",
    "train_corpus = train_corpus.lower()\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "tokens = word_tokenize(train_corpus)\n",
    "tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "dist = FreqDist(tokens_filtered)\n",
    "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rt',\n",
       " 'd',\n",
       " 'хотеть',\n",
       " 'знать',\n",
       " 'ян',\n",
       " 'мочь',\n",
       " 'любить',\n",
       " 'завтра',\n",
       " 'мой',\n",
       " 'хороший']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_filtered_top[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def text_to_sequence(text, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "    for word in tokens_filtered:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "    padding = [0]*(maxlen-len(result))\n",
    "    return padding + result[-maxlen:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"text\"]], dtype=np.int32)\n",
    "x_test = np.asarray([text_to_sequence(text, max_len) for text in df_test[\"text\"]], dtype=np.int32)\n",
    "x_val = np.asarray([text_to_sequence(text, max_len) for text in df_val[\"text\"]], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181467, 40)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(181467, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(df_train[\"class\"], num_classes)\n",
    "y_val = keras.utils.to_categorical(df_val[\"class\"], num_classes)\n",
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 5000\n",
    "max_len = 300\n",
    "num_classes = 2\n",
    "\n",
    "# Training\n",
    "epochs = 30\n",
    "batch_size = 512\n",
    "drop_embed = 0.2\n",
    "dropout = 0.2\n",
    "n_conv = 256\n",
    "k_conv = 3\n",
    "n_dim=64\n",
    "n_dense=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]\n",
    "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}\n",
    "x_train = np.asarray([text_to_sequence(text, max_len) for text in df_train[\"text\"]], dtype=np.int32)\n",
    "x_test = np.asarray([text_to_sequence(text, max_len) for text in df_test[\"text\"]], dtype=np.int32)\n",
    "x_val = np.asarray([text_to_sequence(text, max_len) for text in df_val[\"text\"]], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=n_dim, input_length=max_len\n",
    "                   ))\n",
    "model.add(SpatialDropout1D(drop_embed))\n",
    "model.add(Conv1D(n_conv, k_conv))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(n_dense))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  2/319 [..............................] - ETA: 2:35 - loss: 0.6937 - accuracy: 0.4961WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2670s vs `on_train_batch_end` time: 0.7139s). Check your callbacks.\n",
      "319/319 [==============================] - 91s 284ms/step - loss: 0.5622 - accuracy: 0.6935 - val_loss: 0.5319 - val_accuracy: 0.7232\n",
      "Epoch 2/30\n",
      "319/319 [==============================] - 100s 312ms/step - loss: 0.5112 - accuracy: 0.7381 - val_loss: 0.5268 - val_accuracy: 0.7214\n",
      "Epoch 3/30\n",
      "319/319 [==============================] - 96s 302ms/step - loss: 0.4843 - accuracy: 0.7585 - val_loss: 0.5346 - val_accuracy: 0.7165\n",
      "Epoch 4/30\n",
      "319/319 [==============================] - 96s 300ms/step - loss: 0.4517 - accuracy: 0.7778 - val_loss: 0.5431 - val_accuracy: 0.7216\n",
      "Epoch 5/30\n",
      "319/319 [==============================] - 95s 297ms/step - loss: 0.4162 - accuracy: 0.7983 - val_loss: 0.5653 - val_accuracy: 0.7182\n",
      "Epoch 6/30\n",
      "319/319 [==============================] - 94s 296ms/step - loss: 0.3794 - accuracy: 0.8180 - val_loss: 0.5992 - val_accuracy: 0.7149\n",
      "Epoch 7/30\n",
      "319/319 [==============================] - 93s 292ms/step - loss: 0.3479 - accuracy: 0.8337 - val_loss: 0.6403 - val_accuracy: 0.7169\n",
      "Epoch 8/30\n",
      "319/319 [==============================] - 91s 285ms/step - loss: 0.3220 - accuracy: 0.8458 - val_loss: 0.6900 - val_accuracy: 0.7103\n",
      "Epoch 9/30\n",
      "319/319 [==============================] - 91s 285ms/step - loss: 0.3034 - accuracy: 0.8549 - val_loss: 0.7270 - val_accuracy: 0.7176\n",
      "Epoch 10/30\n",
      "319/319 [==============================] - 90s 282ms/step - loss: 0.2863 - accuracy: 0.8633 - val_loss: 0.7386 - val_accuracy: 0.7115\n",
      "Epoch 11/30\n",
      "319/319 [==============================] - 90s 282ms/step - loss: 0.2746 - accuracy: 0.8695 - val_loss: 0.7757 - val_accuracy: 0.7120\n",
      "Epoch 12/30\n",
      "319/319 [==============================] - 90s 281ms/step - loss: 0.2626 - accuracy: 0.8749 - val_loss: 0.8112 - val_accuracy: 0.7052\n",
      "Epoch 13/30\n",
      "319/319 [==============================] - 90s 282ms/step - loss: 0.2523 - accuracy: 0.8807 - val_loss: 0.8458 - val_accuracy: 0.7101\n",
      "Epoch 14/30\n",
      "319/319 [==============================] - 90s 282ms/step - loss: 0.2449 - accuracy: 0.8838 - val_loss: 0.8560 - val_accuracy: 0.7068\n",
      "Epoch 15/30\n",
      "319/319 [==============================] - 90s 282ms/step - loss: 0.2372 - accuracy: 0.8876 - val_loss: 0.8896 - val_accuracy: 0.7069\n",
      "Epoch 16/30\n",
      "319/319 [==============================] - 91s 285ms/step - loss: 0.2322 - accuracy: 0.8906 - val_loss: 0.8854 - val_accuracy: 0.7076\n",
      "Epoch 17/30\n",
      "319/319 [==============================] - 91s 287ms/step - loss: 0.2255 - accuracy: 0.8931 - val_loss: 0.9065 - val_accuracy: 0.7040\n",
      "Epoch 18/30\n",
      "319/319 [==============================] - 93s 292ms/step - loss: 0.2212 - accuracy: 0.8962 - val_loss: 0.8944 - val_accuracy: 0.7028\n",
      "Epoch 19/30\n",
      "319/319 [==============================] - 94s 295ms/step - loss: 0.2148 - accuracy: 0.8992 - val_loss: 0.9356 - val_accuracy: 0.7034\n",
      "Epoch 20/30\n",
      "319/319 [==============================] - 98s 307ms/step - loss: 0.2105 - accuracy: 0.9003 - val_loss: 0.9287 - val_accuracy: 0.7002\n",
      "Epoch 21/30\n",
      "319/319 [==============================] - 97s 303ms/step - loss: 0.2092 - accuracy: 0.9013 - val_loss: 0.9304 - val_accuracy: 0.7014\n",
      "Epoch 22/30\n",
      "319/319 [==============================] - 100s 313ms/step - loss: 0.2017 - accuracy: 0.9053 - val_loss: 0.9950 - val_accuracy: 0.7019\n",
      "Epoch 23/30\n",
      "319/319 [==============================] - 113s 355ms/step - loss: 0.2017 - accuracy: 0.9054 - val_loss: 0.9851 - val_accuracy: 0.7036\n",
      "Epoch 24/30\n",
      "319/319 [==============================] - 97s 305ms/step - loss: 0.1984 - accuracy: 0.9073 - val_loss: 0.9670 - val_accuracy: 0.6997\n",
      "Epoch 25/30\n",
      "319/319 [==============================] - 99s 311ms/step - loss: 0.1946 - accuracy: 0.9090 - val_loss: 0.9978 - val_accuracy: 0.7000\n",
      "Epoch 26/30\n",
      "319/319 [==============================] - 98s 307ms/step - loss: 0.1911 - accuracy: 0.9108 - val_loss: 1.0805 - val_accuracy: 0.7069\n",
      "Epoch 27/30\n",
      "319/319 [==============================] - 99s 312ms/step - loss: 0.1892 - accuracy: 0.9111 - val_loss: 1.0103 - val_accuracy: 0.7045\n",
      "Epoch 28/30\n",
      "319/319 [==============================] - 98s 308ms/step - loss: 0.1880 - accuracy: 0.9117 - val_loss: 1.0386 - val_accuracy: 0.6996\n",
      "Epoch 29/30\n",
      "319/319 [==============================] - 99s 309ms/step - loss: 0.1859 - accuracy: 0.9126 - val_loss: 1.0034 - val_accuracy: 0.7006\n",
      "Epoch 30/30\n",
      "319/319 [==============================] - 99s 310ms/step - loss: 0.1803 - accuracy: 0.9155 - val_loss: 1.0725 - val_accuracy: 0.7039\n"
     ]
    }
   ],
   "source": [
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "#early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard])#, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 3s 67ms/step - loss: 1.0687 - accuracy: 0.7005\n",
      "\n",
      "\n",
      "Test score: 1.0686968564987183\n",
      "Test accuracy: 0.700524628162384\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_val, y_val, batch_size=batch_size, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 3s 73ms/step\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(x_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод - на последней эпохи результат равен=accuracy: 0.92, что практически соответствует требумому.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ЗАДАНИЕ 2. \n",
    "\n",
    "Предобучаем word2vec и его эмбединга инициализируем сетку, как влияет на качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelW2V_train = Word2Vec(sentences=list(df_train[\"text\"]), size=200, window=5, min_count=1, workers=32, seed=34)\n",
    "x_test = Word2Vec(sentences=list(df_test[\"text\"]), size=200, window=5, min_count=1, workers=32, seed=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57978139, 206345420)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelW2V_train.train(list(df_train['text']),total_examples=len(list(df_train['text'])),epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1bdae81d880>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelW2V_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc):\n",
    "    \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n",
    "    doc = [word for word in doc if word in w2v.wv.vocab]\n",
    "    \n",
    "    return np.mean(w2v[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-97-a3de52c13398>:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  return np.mean(w2v[doc], axis=0)\n"
     ]
    }
   ],
   "source": [
    "w2v=modelW2V_train\n",
    "#df_train['xtrain_count']=df_train['text'].apply(lambda x: document_vector(x))\n",
    "for row in np.arange(len(df_train)):\n",
    "    try:\n",
    "        df_train.at[row,'xtrain_count']= document_vector(df_train.at[row,'text'])\n",
    "    except Exception:        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-839f4da27f4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m history = model.fit( np.asarray(list(df_train['xtrain_count']), dtype=np.int32), y_train,\n\u001b[0m\u001b[0;32m      6\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "#early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "history = model.fit( np.asarray(list(df_train['xtrain_count']), dtype=np.int32), y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard])#, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не получается, непонятно как приводить Word2Vec к нужному типу для вставки как входной параметр в сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
